\chapter{Methodology}
\label{sec:methodology}

This chapter outlines the research method designed to create and assess a multimodal Automatic Personality Recognition (APR) framework for Filipino Instagram users. This framework addresses the specific language and cultural challenges found in the dataset, including code switching and unique patterns of visual self-expression. The process started with preprocessing, followed by exploratory data analysis and feature extraction from text, images, and metadata. It also included an early combination of these features. Finally, machine learning models were trained and thoroughly evaluated to determine how well they performed in predicting users’ personality traits.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{"figures/Methodology-Flowchart.pdf"}
	\caption{Methodology Flowchart}
	\label{fig:methodology_pipeline}
\end{figure}

\section{Data Source}
\label{sec:data}
This research was grounded in the Instagram subset of the PagkataoKo dataset, a comprehensive resource specifically curated for Filipino APR studies \citep{tighe_acorda_2022}. The dataset’s inclusion of user-generated content (posts and images), demographic information, and self-reported Big Five personality scores made it an ideal foundation for this supervised machine learning investigation. The original Instagram subset contained data from 1,380 participants who provided access to their accounts, totaling 195,757 posts.

\subsection{Data Collection Methodology}
The data were gathered between the first week of June 2019 and the second week of February 2020 through a purpose-built web application. This tool streamlined the collection process by first presenting participants with a consent form and study directions. Upon granting consent, participants chose whether to authorize the application to access their social media data from Instagram only, Twitter only, or both. Following this automated collection, participants completed a demographic questionnaire and the 44-item Big Five Inventory (BFI-44) to assess their personality traits.

The application used the official Instagram Legacy API and Twitter API v1. For Instagram, the application collected post data (captions and image links) from as many posts as the API returned, along with a link to the profile picture and account metadata. Only links to images were collected to reduce strain on the application. If a post contained multiple photos, only the link to the first photo was collected, while posts containing videos were discarded. Due to the observed behavior of image URLs changing over time, the images from the collected links were downloaded at the end of each day of collection to avoid losing access \citep{tighe_acorda_2022}.

\subsection{Participant Sampling and Filtering}
A mixed sampling strategy was employed to recruit participants. The initial phase utilized convenience and snowball sampling, in which information about the study was disseminated within the researchers’ immediate networks. To reach a broader Filipino audience, this was followed by a series of targeted online advertisement campaigns on Facebook, Instagram, and Twitter.

From an initial pool of 3,186 participants, a filtering process was applied to ensure relevance to the Filipino context. Only individuals who identified their nationality as either “Filipino” or “mixed-Filipino” were retained, resulting in a final cohort of 3,128 individuals whose data comprised the full PagkataoKo dataset \citep{tighe_acorda_2022}.


\subsection{Data Characteristics before Filtering}
This study was only concerned with the Instagram portion of the dataset, which comprised 44.1\% of the data, or 1,380 respondents who authorized access to their accounts. The posts collected were made over a 10-year period prior to the data collection (2010–2020) \citep{tighe_acorda_2022}. Detailed numerical characteristics of this subset are shown in Table~\ref{tab:pagkataoko_data_characteristics}. Further details, such as participant demographics and language use, were discussed in Section~\ref{subsec:eda}.

\subsection{Dataset Characteristics After Filtering}
\label{subsec:final_data_characteristics}

The Instagram subset underwent additional filtering to ensure data quality and coherence. Only users with a minimum of 30 posts containing both images and captions were retained. Additionally, a maximum of 300 of each user's most recent posts were included in the final analysis.

The characteristics of the final filtered dataset are presented below:

\begin{itemize}
	\item \textbf{Total Users:} 752 unique Filipino Instagram users
	\item \textbf{Total Posts:} 139,713 posts (all containing both images and captions)
	\item \textbf{Average Posts per User:} 185.79 posts (SD = 217.34)
	\item \textbf{Posts per User Range:} 30 to 1,740 posts
	\item \textbf{Data Split Distribution:} Training set: 526 users (70\%), Validation set: 112 users (15\%), Test set: 114 users (15\%)
\end{itemize}

All posts in this final dataset contain both textual captions and images. The 300-post limit per user ensures that users with extensive posting histories do not disproportionately influence the feature representations, while the 30-post minimum ensures sufficient data for reliable user-level aggregation.

\begin{table}[h!]
\centering
\caption{Data characteristics of user-generated and account-related data across the Instagram subset of the PagkataoKo dataset}
\label{tab:pagkataoko_data_characteristics}
\begin{tabular}{@{}llr@{}}
\toprule
\textbf{Data} & \textbf{Statistics} & \textbf{Count} \\ 
\midrule
    {Collected Posts} & Total & 195,757 \\
    & Average & 141.85 \\
    & SD & 224.00 \\
    & Min / Max & 0 / 1,902 \\
    & \# w/ Caption & 178,650 \\
    & \# w/ Image & 162,500 \\
\midrule
    {Acct. Recorded Posts} & Average & 146.60 \\
    & SD & 267.99 \\
    & Min / Max & 0 / 5,680 \\
\midrule
    {Following Count} & Average & 470.39 \\
    & SD & 442.85 \\
    & Min / Max & 0 / 6,338 \\
\midrule
Profile Pictures & Total & 1,030 \\
\bottomrule
\end{tabular}
\begin{minipage}{\linewidth}
\small\textit{Note.} Adapted from \textit{Data characteristics of user-generated and account-related data across three participant subsets (Table 3)} \citep{tighe_acorda_2022}.
\end{minipage}
\end{table}


\subsection{Personality Score Transformation to Labels}
In its raw form, the PagkataoKo dataset provides personality scores as continuous numerical values ranging from 1.0 to 5.0 for each of the Big Five traits. Since this study framed personality recognition as a classification task, these continuous scores were transformed into discrete labels.

To achieve this, a \textbf{median split} was performed independently for each of the five personality traits. This choice was informed by the findings of \citet{aviles2023}, who explored eight different discretization methods on Filipino social media data. Their study found that while methods that removed a large number of participants (such as LHNASD, which removed 70\% of the data) achieved the highest performance, the LH Median method provided the best balance between performance and data retention. Specifically, the LH Median method achieved the second-highest average Kappa score while only removing a small fraction of participants (approximately 6\%) whose scores fell exactly on the median \citep{aviles2023}. Given this trade-off, the median split was selected as a robust and practical approach that preserved the vast majority of the dataset while still creating distinct high and low classes for the models.

For a given trait (e.g., Openness), users with a score above the median value for that trait were assigned the High label (e.g., High-Openness), while users with scores at or below the median were assigned the Low label (e.g., Low-Openness). This process resulted in five separate binary labeling schemes, one for each personality dimension—which served as the target variables for the classification models.

\begin{figure}[H]
	\centering
	% Placeholder for the actual diagram image
	\includegraphics[width=1\textwidth]{"figures/Scores.png"}
	\caption{Category to Label via Median Split}
	\label{fig:median_split_diagram}
\end{figure}


\subsection{Data Splitting}
For model training and evaluation, the dataset was divided using a user-level stratified split: \textbf{70\%} for training, \textbf{15\%} for validation, and \textbf{15\%} for testing. A user-level split was used to ensure that data of users remains completely "unseen" prior to validation or testing. To ensure that the proportion of High and Low labels for each personality trait remained consistent across these sets, the split was stratified based on the generated personality labels. This approach helped prevent model bias and ensured that the models were trained and evaluated on representative distributions of each personality class.

\section{Preprocessing}
This stage cleaned and organized the raw data to make it suitable for feature extraction. The main task was \textbf{user-level aggregation}, in which all posts, images, and metadata for a single user were collected into a single behavioral profile. 

\subsection{Image Preprocessing}
A maximum of 300 of a user's most recent images were collected for use in the study due to resource constraints. All collected images then went through preprocessing to prepare them for the \textbf{ResNet50} model. Each image was resized to 224×224 pixels to match the model’s required input size. Corrupted image files were also checked, and any posts with unreadable images were flagged and excluded from the image-based feature extraction process.

\subsection{Text Preprocessing}
A basic text cleaning method was used to preserve as much of the original content as possible, including multilingual text. The process involved breaking down the raw caption text into tokens and replacing elements such as URLs and user mentions with generic tokens (e.g., URL, USER) rather than removing them completely. This approach preserved the context of where a link or mention appeared. All languages were retained in the text. Furthermore, all captions of a single user are aggregated into a single vector using two different aggregation methods. This aggregation is further discussed in Section~\ref{subsec:wordembeddingrepresentation}.

\subsection{Metadata Preprocessing}
The two raw numerical features, PostCount and FollowingCount, are normalized using Min-Max scaling. This converts them to a consistent 0.0-1.0 range.


\section{Exploratory Data Analysis}
\label{subsec:eda}

Before feature extraction, exploratory data analysis (EDA) was conducted to understand the key characteristics of the dataset. This analysis aimed to identify demographic trends and content patterns that could guide feature engineering and model interpretation.

The demographic distribution of the final 1,300-user subset, as reported in the PagkataoKo dataset paper, was summarized in Table 4.1. The data showed a user base that was predominantly young (49.3\% aged 18–20) and female (78.0%).

As detailed by \citet{tighe_acorda_2022}, the language of each post was determined using a majority vote between two language identifiers (Polyglot and FastText). Posts where the identifiers agreed were labeled as either English (75.6\%) or Tagalog (4.3\%). Posts where the identifiers did not agree were assigned a Conflict label (19.4\%), a category understood to primarily represent posts containing code-switching or other forms of mixed-language use.

\begin{table}[h]
	\centering
	\caption{Demographic distribution of Instagram subset (n=1,300)}
	\label{tab:demo}
	\begin{tabular}{lcc}
		\hline
		\textbf{Characteristic} & \textbf{Category} & \textbf{Percentage} \\ \hline
		\textbf{Age} & 18-20 & 49.3\% \\
		& 21-23 & 31.3\% \\
		& 24-26 & 10.7\% \\
		& $\geq$27 & 8.8\% \\ \hline
		\textbf{Sex} & Female & 78.0\% \\
		& Male & 20.0\% \\
		& Intersex/ Declined & 2.0\% \\ \hline
		\textbf{Language} & English & 75.6\% \\
		& Tagalog & 4.3\% \\
		& Code-switched & 19.4\% \\ \hline
	\end{tabular}
\end{table}
% --- REVISION G START: Added new section for PCA Rationale ---
\section{Feature Engineering}
\label{subsec:features}
This stage converted the raw, multimodal data associated with each Instagram user—comprising images, textual captions, and account metadata—into a single, quantitative feature vector. This vector was designed to be processed by supervised machine learning algorithms (logistic regression, support vector machines, and XGBoost) for the task of personality trait prediction. The study primarily employed \textbf{average pooling}, which involved computing the mean of all feature vectors for a given user. This aggregation strategy was deliberately chosen to align with the theoretical view of personality as a stable, long-term construct \citep{babcock2020big}. By averaging a user's content features over many posts, the resulting vector represented their typical behavior and expressive style, effectively smoothing out outlier content that might not have been representative of their core personality \citep{azucar_predicting_2018}. This approach aimed to capture the consistent 'trait' aspect of a user, rather than a transient emotional 'state' that might be reflected in a single post.

The process was divided into three parallel pipelines, one for each data modality.

\subsection{Image Modality Pipeline}
This pipeline processed all valid images from a user's profile to extract a comprehensive set of visual features. For all image features, only the 300 most recent images of a user were processed. For each image, the following features were extracted in parallel before being aggregated to the user level.

\subsubsection{High-Level Semantic Features via ResNet50}
The pre-trained \textbf{ResNet50} convolutional neural network was used as a feature extractor. For each image, a \textbf{2,048-dimensional feature vector} was extracted from the output of the final average pooling layer. This vector captured abstract visual concepts, objects, and textures.

ResNet50 was chosen because it demonstrated higher classification accuracy on benchmark datasets like ImageNet, was substantially more efficient in terms of model size (98 MB vs. 549 MB for VGG19), and its residual connections allowed for the effective training of deeper networks. This enabled the model to learn more complex feature representations without suffering from the vanishing gradient problem that affected older architectures \citep{he2015, simonyan2014very}.

\subsubsection{Object and Scene Recognition via the Imagga API}
To quantify the semantic content and social context of the images, this study leveraged the \textbf{Imagga API}, a commercial computer vision service.

Imagga is a cloud-based service that provides a suite of powerful image analysis tools through a simple API. Its core functions include automated image tagging, categorization, color extraction, and content moderation, all powered by pre-trained deep learning models capable of recognizing thousands of objects, scenes, and concepts \citep{imagga_website, imagga_solutions}. The Imagga API was selected for its robust recognition capabilities, which provided a reliable method for extracting semantic features without the significant overhead of training a custom object detection model. Its structured JSON output is easily parsed and integrated into the feature engineering pipeline, making it a pragmatic and efficient choice for this research \citep{imagga_docs}.

% While the Imagga API’s general model is capable of recognizing over 3,000 distinct objects and concepts \citep{imagga_solutions}, creating a feature vector of this size for each user would have been computationally inefficient and resulted in a highly sparse feature space. To create a denser and more manageable feature set, a fixed-size vocabulary of the most relevant tags was established. This followed a standard approach analogous to a bag-of-words model in text processing \citep{salton1975}. A vocabulary of the top N=200 most frequent tags across the entire dataset was identified. Subsequently, for each user, a \textbf{200-dimensional feature vector} was constructed, where each dimension represented the frequency of one of these top tags appearing in that user’s posts.

Lemmatization was applied using a 94-term dictionary, using the WordNet corpus in the Natural Language Toolkit (NLTK) \citep{bird2009natural} in order to normalize variations in wording. For example, plural words such as "animals" were reduced to simply "animal". This reduced vocabulary redundancy.

The study specifically utilized the \texttt{/tags} endpoint of the Imagga API. For each image submitted, the API returned a JSON object containing a list of detected tags (e.g., “person,” “selfie,” “food,” “beach”) and a corresponding confidence score (from 0 to 100) for each tag \citep{imagga_docs}. These tags were used to construct features such as the frequency of posts containing people, selfies, or specific objects, which prior literature had linked to traits like Extraversion. Preprocessing was done on tags that contained spaces, which were replaced with underscores. For example, "portrait photography" was replaced with "portrait\_photography".

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{"figures/imagga_example_image.png"}
		\subcaption*{Example Input Image}
	\end{minipage}
	\hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\begin{verbatim}
			{
				"result": {
					"tags": [
					{ "confidence": 99.2, 
						"tag": { "en": "food" } },
					{ "confidence": 98.7, 
						"tag": { "en": "dish" } },
					{ "confidence": 97.3, 
						"tag": { "en": "cuisine" } },
					{ "confidence": 95.1, 
						"tag": { "en": "delicious" } },
					{ "confidence": 92.8, 
						"tag": { "en": "meal" } },
					{ "confidence": 89.5, 
						"tag": { "en": "restaurant" } }
					]
				}
			}
		\end{verbatim}
		\subcaption*{Corresponding Imagga API JSON Output}
	\end{minipage}
	\caption{Example image and corresponding JSON output from the Imagga API /tags endpoint. The left image shows a sample photograph (source: Imagga documentation), and the right panel displays the API's JSON response containing detected tags with their confidence scores.}
	\label{fig:imagga_json}
\end{figure}

Furthermore, the usage of Imagga is also for feasibility purposes due to the Imagga JSON tags already being a part of the PagkataoKo Dataset.

For each user, the tag lists from all their images were aggregated to compute the frequency of each tag across the user's posts. Two versions of these tag features were generated: one using all tags (no confidence filter) and one applying a 30\% confidence threshold (i.e., keeping only tags with a confidence score of at least 30\%). Preliminary experiments on the validation set showed that the confidence‑filtered version resulted in near‑zero predictive performance (\(\kappa \approx 0\)) for all traits, while the unfiltered version retained meaningful variance. Therefore, only the unfiltered Imagga features (i.e., all tags) were used in the final feature sets. This choice is also practical because the raw Imagga JSON tags are already part of the PagkataoKo dataset \citep{tighe_acorda_2022}.

\subsubsection{Low-Level Photo Characteristics Analysis}
To capture the aesthetic qualities of images, a set of low-level features was extracted.

For each image, color features were derived using the HSV (Hue, Saturation, Value) color space. The inclusion of these features was based on the works of \citet{ferwerda2016} and \citet{Ferwerda2018}, whose studies were similar to ours in that they conducted APR using Instagram photos and found correlations between certain features and Big Five personality traits. These features were also employed in the work of \citet{Branz2020}, who similarly focused on Instagram photo-based APR. 

\begin{itemize} 
	\item \textbf{Hue-related Features:} The range of the Hue (H) parameter is divided into six intervals corresponding to the primary hues: red, orange, yellow, green, blue, and violet. For each hue, the share of the image surface it covers is calculated by dividing the number of pixels in that hue's interval by the total number of pixels. Furthermore, these are merged into composite features representing the share of \textit{warm colors} (red, orange, yellow) and \textit{cold colors} (green, blue, violet). 
	\item \textbf{Saturation-related Features:} For each image, the mean saturation and saturation variance are calculated. The saturation axis is also divided into three equally spaced intervals (low, mid, high), and the share of pixels falling into each interval is computed. These features quantify whether an image's colors are bleak and colorless (low saturation), vivid (high saturation), or a mix of both (high variance). 
	\item \textbf{Value-related Features:} The mean and variance of the Value (V) parameter are calculated across all pixels to represent how light or dark an image is and its level of contrast, respectively. Similar to saturation, the value axis is divided into three intervals (low, mid, high), and the share of pixels in each is calculated to quantify the presence of dark and light areas. 
	\item \textbf{Pleasure-Arousal-Dominance (PAD) Features:} To model the emotional expression conveyed by an image's color properties, the PAD model by \citet{Valdez1994} is adopted. This model computes three composite features as a linear combination of the average Value (V) and Saturation (S) levels: 		
		\begin{itemize} 
			\item Pleasure = $0.69 \times V + 0.22 \times S$ 
			\item Arousal = $-0.31 \times V + 0.60 \times S$ 
			\item Dominance = $-0.76 \times V + 0.32 \times S$ 
		\end{itemize} 
\end{itemize} 

All of these post-level aesthetic features are then aggregated to the user level by calculating their average values across all of a user's images, creating a comprehensive aesthetic profile.

The library \textit{cupy} \citep{Okuta2017CuPyA} was used in order to convert image RGB data into HSV data. The library divided the hues into the six classifying colors and additionally divided those colors into warm and cool colors. The PAD values were then calculated using the converted data.


\subsection{Text Modality Pipeline}
This pipeline processed all textual captions from a user's posts to extract linguistic and semantic features.

\subsubsection{Contextual Embeddings from TF-IDF}
To capture deep contextual meaning from the user's captions, this study utilized the \textbf{TF-IDF} model to generate embeddings for the text. Its training corpus included the text captions of 178,650 Instagram posts collected in the Pagkatao dataset \citep{tighe_acorda_2022}. For each user's aggregated captions, the TF-IDF model produced a sparse feature vector. The dimensionality of this vector is equal to the vocabulary size of the training corpus. The resulting feature vector came out to be 36,398 dimensions.

\begin{comment}
%While traditional methods like TF-IDF capture lexical importance, they fail to understand context, semantics, or word order. BERT (Bidirectional Encoder Representations from Transformers) models do well in natural language understanding and have been shown to significantly outperform TF-IDF in personality classification tasks. For example, a study by \citet{zhang2023} directly comparing the two approaches for MBTI personality prediction found that a fine-tuned BERT model paired with a Logistic Regression classifier achieved an average accuracy of 87.7\%, a notable improvement over the 84.3\% accuracy achieved by the same classifier using a traditional TF-IDF vectorizer \citep{zhang2023}. BERT's key advantage is its ability to generate contextual embeddings; the representation of a word changes based on the words surrounding it. Given the dataset's mix of English, Tagalog, and code-switching, a multilingual BERT model is particularly suitable, as it has been pre-trained on a corpus of over 100 languages and can effectively handle the linguistic complexity of the data \citep{devlin2018bert, cruz2022roberta}.
\end{comment}

\subsubsection{Word Embedding Representation}
\label{subsec:wordembeddingrepresentation}
As a complementary feature to the contextual embeddings from TF-IDF, pre-trained \textbf{Word2Vec} word embeddings were also used to capture a static, non-contextual representation of semantic meaning \citep{Mikolov_Sutskever_Chen_Corrado_Dean_2013}. For each user, the embeddings for all words in their captions were combined with two averaging methods. Each resulted in a single, dense vector for each user that represented the aggregate meaning of their posts.

The first averaging method, dubbed "All words equal," first concatenated all captions of a user into one large caption before converting each word into a vector using Word2Vec. Each vector of each word was then averaged in order to obtain the final user vector. This resulted in a vector where each word that the user used has an equal weight in the final vector. The second averaging method, dubbed "All posts equal," placed more emphasis on the overall composition of each post or caption as a whole rather than individual words. This approach treats each post as separate until the final user aggregation. This is done by first obtaining the Word2Vec vector of each word, still separated by post, and then getting the average vector for all words in a single post. The vector of each post under a single user is then averaged, resulting in a final user vector. In this approach, the weights of individual words are more skewed in order to place emphasis on overall meanings.

Both averaging strategies were evaluated independently on the validation set for each personality trait. The strategy that yielded the higher validation Kappa was retained for that trait in the subsequent multimodal experiments. This allowed the model to adapt the text representation to the trait‑specific linguistic patterns.

\begin{figure}[H]
	\centering
	% Placeholder for the actual diagram image
	\includegraphics[width=1\textwidth]{"figures/word2vec_averaging.png"}
	\caption{Visualization of Word2Vec averaging methods}
	\label{fig:word2vec_averaging}
\end{figure}

The pre-trained model used was from \citet{velasco2021filwembs}, which contained “texts from Tagalog Wikipedia, news articles, and tweets.” Its data sources included \citet{Cruz_Cheng_2019}’s WikiText-TL-39, \citet{Cruz_Resabal_Lin_Velasco_Cheng_2021}’s NewsPH-NLI, and an unpublished Twitter dataset. Preprocessing techniques, such as “removing quotes and its contents” \citep{velasco2021filwembs}, were applied prior to performing word embedding.


\subsection{Metadata Modality Pipeline}
Behavioral signals were extracted from user account metadata. As noted in the PagkataoKo dataset paper, the Instagram subset provided two key account-level metrics: the total number of posts a user had made (\texttt{PostCount}) and the number of other accounts they followed (\texttt{FollowingCount}) \citep{tighe_acorda_2022}.

This pipeline involved two key steps:
\begin{enumerate}
	\item \textbf{Feature Extraction:} The raw numerical values for \texttt{PostCount} and \texttt{FollowingCount} are extracted for each user.
	\item \textbf{Normalization:} These features exist on vastly different scales; for example, a user might have 100 posts but follow 1,000 accounts. To prevent algorithms that are sensitive to feature magnitude (such as Support Vector Machines) from giving undue weight to features with larger numerical ranges, all metadata features are scaled to a fixed range of  using min-max normalization.
\end{enumerate}
This preprocessing step is for ensuring that the metadata features contribute fairly to the final model alongside the features from the image and text modalities. The final output of this pipeline is a 2-dimensional, normalized metadata feature vector for each user.

\subsection{Dimensionality Reduction as an Optional Step}
The primary methodological approach of this study was to utilize the full, high-dimensional feature vectors generated from ResNet50 (2,048 dimensions) and TF-IDF (vocabulary size). This decision was motivated by a core research objective: to maintain the highest possible level of interpretability. Applying dimensionality reduction techniques, such as PCA, transforms the original features into a new set of components, which can make it difficult to trace a prediction back to the specific, original visual or textual cues (e.g., a particular object in an image or a specific word in a caption).

However, if initial modeling experiments with the full feature set yielded poor performance, exhibited significant signs of overfitting, or proved computationally prohibitive, \textbf{Principal Component Analysis (PCA)} was explored as an optional secondary step. In this contingency, PCA would be applied to the aggregated ResNet50 and TF-IDF feature sets to create a more compact and less noisy representation before feature fusion. This two-stage approach allowed the study to first prioritize interpretability while retaining a robust method for managing dimensionality if it proved necessary.

\subsection{Feature Pipelines}
Figures \ref{fig:image_pipeline_diagram}, \ref{fig:text_pipeline_diagram}, and \ref{fig:fusion_diagram} illustrate the flow of data from its raw form through the parallel feature engineering pipelines to the final aggregated user vector.

The final value of $k$ for each feature set was determined empirically during implementation. 

\begin{figure}[H]
	\centering
	% Placeholder for the actual diagram image
	\includegraphics[width=\textwidth]{"figures/Image-Pipeline-Diagram.png"}
	\caption{The Image Feature Engineering Pipeline. }
	\label{fig:image_pipeline_diagram}
\end{figure}

Features were extracted from each image via parallel processes capturing three distinct levels of data: High-Level Semantics (ResNet50), Object Content (Imagga API), and Low-Level Aesthetics (HSV and Color Properties). These post-level features were then aggregated to the user level and concatenated. 

As illustrated in Figure \ref{fig:image_pipeline_diagram}, the final user-level image vector with 30\% confidence filtering resulted in a total dimensionality of approximately 2,807. This vector is composed of:
\begin{itemize}
	\item \textbf{2,048} dimensions from the ResNet50 model;
	\item \textbf{738} dimensions from the filtered Imagga tags;
	\item \textbf{21} dimensions from the HSV and color aesthetic features.
\end{itemize}

\begin{figure}[H]
	\centering
	% Placeholder for the actual diagram image
	\includegraphics[width=\textwidth]{"figures/Text-Pipeline-Diagram.png"}
	\caption{The Text Feature Engineering Pipeline. }
	\label{fig:text_pipeline_diagram}
\end{figure}

A user's entire caption corpus was processed through two parallel paths to extract lexical features (TF-IDF) and semantic embeddings (Word2Vec). These features were aggregated to the user level and concatenated. 

As shown in Figure \ref{fig:text_pipeline_diagram}, the TF-IDF path involved average pooling using the top 1,500 terms. The final output was a user-level text vector with a dimensionality of 1,800, calculated as the sum of:
\begin{itemize}
	\item \textbf{1,500} dimensions from the top-term TF-IDF vectorizer;
	\item \textbf{300} dimensions from the Word2Vec embeddings.
\end{itemize}


\begin{figure}[H]
	\centering
	% Placeholder for the actual diagram image
	\includegraphics[width=\textwidth]{"figures/Fusion-Diagram.pdf"}
	\caption{User-Level Aggregation and Feature Fusion.}
	\label{fig:fusion_diagram}
\end{figure}

The outputs from the individual modality pipelines—the Image Vector (Est. Dim: \textasciitilde2,269), Text Vector (Est. Dim: \textasciitilde V (Vocabulary Size)), and Metadata Vector (Dim: 2)—are combined via feature concatenation. This process results in the final, unified user feature vector which serves as the input for the machine learning models.

\section{Feature Selection}
\label{sec:feature_selection}

For each modality, multiple feature extraction variants were available (e.g., different TF‑IDF vocabulary sizes, two Word2Vec aggregation methods, two Imagga tag‑filtering options). Rather than applying a generic feature selection algorithm (such as Chi‑square or LASSO), we adopted a \textit{configuration selection} approach: for every personality trait, we evaluated each variant on the validation set and chose the one that maximized Cohen's Kappa. This procedure ensured that the final feature representation was tailored to the trait while keeping the test set completely unseen.

Specifically, the following variants were considered:

\begin{itemize}
	\item \textbf{TF‑IDF:} Two versions—one using all terms (vocabulary size 36,398) and one using only the top 1,500 most frequent terms. The latter reduces dimensionality and may improve generalization.
	\item \textbf{Word2Vec:} Two aggregation strategies—\textit{words‑weighted‑equally} (averaging all word vectors across all captions) and \textit{posts‑weighted‑equally} (averaging per post then across posts), as described in Section~\ref{subsec:wordembeddingrepresentation}.
	\item \textbf{Imagga tags:} Two versions—unfiltered (all tags) and confidence‑filtered (retaining only tags with confidence \(\geq 30\%\)). The confidence‑filtered version gave negligible validation performance and was therefore discarded.
\end{itemize}

For each trait, the best‑performing configuration of each modality (based on validation Kappa) was selected. These optimal configurations were then used to build the unimodal, bimodal, and multimodal feature sets. Because the selection was guided solely by validation performance, the test data remained untouched.

After fixing the configuration for each trait, we did \textit{not} perform any further feature pruning (e.g., by coefficient magnitude or correlation filters). 

\section{Early Feature Fusion}
\label{subsec:fusion}
This stage employed \textbf{early fusion} (also known as feature-level fusion) to combine the processed features from the image, text, and metadata pipelines into a single, unified representation for each user. The final user-level vectors from each of the three modality pipelines were concatenated to create a single, high-dimensional feature vector. This unified vector served as the final input for the machine learning models.

\section{Model Training}
\label{subsec:models}

Three distinct classification models were trained to predict the Big Five personality traits: Logistic Regression, a Support Vector Machine (SVM), and an XGBoost classifier. These models were implemented using the \texttt{scikit-learn} \citep{pedregosa2011} and \texttt{XGBoost} \citep{chen2016} Python libraries.

A logistic regression (LR) model was trained as a strong and interpretable baseline. Its performance provided a benchmark against which the more complex models could be compared, making it easier to measure improvements.

A Support Vector Machine (SVM) was trained, and its kernel function (e.g., Radial Basis Function (RBF), linear, polynomial) and regularization parameters (\texttt{C, gamma}) were optimized through a comprehensive grid search to identify the best configuration for the data.

Lastly, an XGBoost classifier was also trained. This gradient-boosting algorithm effectively handled different types of features and included a built-in method to assess the importance of those features.

\section{Model Analysis}
\label{subsec:analysis}
A detailed review was conducted to assess the predictive performance of the best model per trait. The primary metric was \textbf{Cohen's Kappa} (\(\kappa\)), which measures agreement between predicted and actual labels while accounting for chance. Kappa is particularly suitable for binary classification tasks with potentially imbalanced classes, as it provides a more robust assessment than raw accuracy.

Beyond predictive performance, we aimed to understand \textit{which} features drive the models’ decisions for each personality trait. Two complementary analysis techniques were employed:

\begin{itemize}
	\item \textbf{Feature importance from model coefficients and gain.} For logistic regression, the learned coefficients directly indicate the direction (positive/negative) and magnitude of each feature’s influence on the log‑odds of the High label. For XGBoost, we extracted the gain‑based feature importance, which sums the improvement in accuracy brought by a feature whenever it is used in a tree split \citep{chen2016}. Both measures provide a global ranking of the most influential visual, textual, and metadata cues.
	\item \textbf{Pearson correlation.} To examine linear relationships between individual features and the binary personality labels, we computed the point‑biserial correlation coefficient (equivalent to Pearson’s \(r\)). This analysis was applied to continuous features such as the ResNet50 dimensions, HSV statistics, and metadata counts.
\end{itemize}

Lastly, a series of model analyses were conducted. This involved training and evaluating separate models on distinct subsets of the full feature set. The same set of classifiers (Logistic Regression, SVM, XGBoost) was applied to each experimental condition to ensure a fair comparison. The experimental conditions were explicitly defined in Table \ref{tab:ablation_conditions}.

\begin{table}[H]
	\centering
	\caption{Experimental Conditions for Modality Analyses Studies}
	\label{tab:ablation_conditions}
	% Use tabularx to make the table fit the text width.
	% The X column type will automatically expand and wrap text.
	\begin{tabularx}{\textwidth}{l c c c X} 
		\hline
		\textbf{Model Name} & \textbf{Image} & \textbf{Text} & \textbf{Meta} & \textbf{Description} \\ \hline
		Text-Only & & \checkmark & & Unimodal baseline (TF-IDF, Word2Vec) \\
		Image-Only & \checkmark & & & Unimodal baseline (ResNet50, Imagga, Aesthetics) \\
		Metadata-Only & & & \checkmark & Unimodal baseline (Post/Following Count) \\
		Text + Image & \checkmark & \checkmark & & Bimodal synergy of visual and linguistic cues \\
		Text + Metadata & & \checkmark & \checkmark & Bimodal value of metadata with text \\
		Image + Metadata & \checkmark & & \checkmark & Bimodal value of metadata with images \\
		\textbf{Full Multimodal} & \checkmark & \checkmark & \checkmark & The complete proposed model \\ \hline
	\end{tabularx}
\end{table}
