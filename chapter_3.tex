%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Filename    : chapter_3.tex 
%
%   Description : This file will contain your Theoretical Framework.
%                 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Theoretical Framework}
\label{sec:theoframework}

This chapter presents the theoretical foundations that inform the development of a multimodal personality recognition framework for Filipino Instagram users. The discussion is organized around key technical pillars, including text and image processing, feature fusion techniques, machine learning theory, and the underlying personality psychology model.

\section{Text Processing Theory}

Textual data from Instagram captions provides valuable linguistic cues for personality inference. This study employs the \textit{Vector Space Model} \citep{salton1975}, where text is represented as weighted feature vectors using the Term Frequency Inverse Document Frequency (TF-IDF) scheme. TF-IDF reflects the importance of words within individual captions while down weighting common terms across the corpus, enhancing the model's ability to capture personality relevant language patterns.


TF-IDF is a feature selection model that "measures how often a word was used by a given personality type as compared to all the personality type" It is best used in "informal text data" as it takes into account the whole body of text when measuring the frequency of the word given the personality type \citep{Pradhan_Bhansali_Chandnani_Pangaonkar_2020}.

Beyond sparse TF-IDF vectors, word embeddings such as Word2Vec or contextual models like BERT offer dense semantic representations that capture nuanced word relationships. These embeddings complement TF-IDF by providing richer contextual information, essential for modeling bilingual and informal language prevalent among Filipino social media users.

Word2Vec learns dense vector representations of words, capturing semantic and syntactic meanings based on training on large text corpora. It provides accurate representations at lower computational costs with simpler architecture, which is ideal for larger datasets such as the one we plan to work with.  \citep{Ma_Zhang_2015}. It can model “linguistic regularities and patterns” \citep{Mikolov_Sutskever_Chen_Corrado_Dean_2013}, which is useful for the study as the dataset to be used contains informal language, slang, typos, and morphologically rich Tagalog words. 


\section{Image Processing Theory}

Instagram's visual content necessitates effective image processing techniques to extract personality related features. Standard preprocessing steps include resizing and normalization to ensure uniformity across image inputs. Feature extraction leverages deep Convolutional Neural Networks (CNNs), specifically the ResNET50 architecture, which is recognized for its high accuracy on benchmark datasets and its ability to learn complex representations by training deeper networks without suffering from the vanishing gradient problem \citep{he2015}.

Extracted image features encompass both low level characteristics (e.g., color, brightness) and high level semantic content (e.g., objects, scenes) known to correlate with personality traits, as supported by prior research.

\section{Statistical Evaluation Theory}
To assess the significance and strength of observed relationships in the data, this study employs statistical tests suited for categorical variables.

\subsection{Chi-squared Test of Independence}
The \textbf{Chi-squared ($\chi^2$) test of independence} is a test that is used to calculate significant differences between groups. The formula is as follows:
$$\sum\chi^2_{i-j} = \frac{(O - E)^2}{E}$$
When calculating Chi-squared, data is thought of as being in table format. The \textit{i-j} notation indicates that the $\chi^2$ value must be calculated for each cell, with $i$ being the very first cell and $j$ being the last. For each cell, $O$ is the observed value and $E$ is the expected value, which is calculated with this formula:
$$E = \frac{ \text{M}_R  \times  \text{M}_C }{\text{n}}$$
$\text{M}_R$ refers to the \textit{row marginal} of the cell, which means the sum of all values in that row. Likewise, $\text{M}_C$ refers to the \textit{column marginal} of the cell, which is the sum of all values in the column. $n$ refers to the total sample size. At this point, some interpretations can be made. Cell $\chi^2$ values below 1.0 mean that the expected value is roughly equal to the observed value. Higher values mean that the observed values are more or less than expected.

Once all $\chi^2$ value are obtained, they will be summed to get the final $\chi^2$ value for the whole table. In order to determine the significance of this statistic, the \textit{degrees of freedom} is calculated using this formula:
$$df = (Number of rows - 1)(Number of columns - 1)$$
With both the $\chi^2$ value and \textit{degrees of freedom}, the \textit{Chi-square table of significances} is referred to in order to obtain the \textit{significance level} of the value. If $P < 0.05$, it can be concluded that the groups are significantly different. For further interpretation, the most common test used to assess the strength of this significance and correlation is the \textit{Cramer’s V test}, calculated with this formula:
$$V = \sqrt{ \frac{ \chi^2 }{ n(k - 1) } }$$
In this, $k$ refers to the smaller value between $(Number of rows - 1)$ and $(Number of columns - 1)$. It is a standard association metric ranging from $0-1$ where values closer to $0$ indicate weaker correlations and values closer to $1$ indicate stronger correlations.

\section{Fusion Techniques}

Multimodal learning integrates heterogeneous features from text and images to improve personality prediction accuracy. Two primary fusion strategies are considered:

\begin{itemize}
	\item \textbf{Early Fusion:} Feature level integration, where extracted features from each modality are concatenated into a unified vector prior to model training.
	\item \textbf{Late Fusion:} Decision level integration, where separate models are trained per modality, and their outputs are combined for final prediction.
\end{itemize}

Following \citet{liu2022} and \citet{kampman2018},where feature vectors from all modalities are concatenated into a single, unified vector. This approach is based on the rationale that certain traits (e.g., Openness) may be more visually encoded, while others (e.g., Neuroticism) are better captured through language, and allows the model to learn from all available cues.

\section{Machine Learning Theory}

To model the relationship between extracted multimodal features and personality traits, this study employs established supervised learning algorithms:

\subsection{Logistic Regression}
\textbf{Logistic Regression (LR)} is a linear model used for binary classification. It calculates the probability of an instance belonging to a class using the sigmoid function, which maps a linear combination of features to a value between 0 and 1. The probability is given by:
$$P(y=1 | \mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w} \cdot \mathbf{x} + b)}}$$
where $P(y=1|\mathbf{x})$ is the probability of the positive class given the input feature vector $\mathbf{x}$, $\mathbf{w}$ represents the feature weights, and $b$ is the bias term.

\subsection{Support Vector Machines}
\textbf{Support Vector Machines (SVMs)} are well-suited for high-dimensional feature spaces. For classification, an SVM seeks to find the optimal hyperplane that best separates the data points of different classes by maximizing the margin between them. The optimization problem for a soft-margin SVM is formulated as:
$$\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i$$
subject to the constraints $y_i(\mathbf{w} \cdot \mathbf{x}_i - b) \geq 1 - \xi_i$ and $\xi_i \geq 0$. Here, $C$ is a regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors, represented by the slack variables $\xi_i$. SVMs can capture non-linear relationships using the kernel trick, where the dot product is replaced by a kernel function, $K(\mathbf{x}_i, \mathbf{x}_j)$.

\subsection{Extreme Gradient Boosting (XGBoost)}
\textbf{Extreme Gradient Boosting (XGBoost)} is a powerful tree-based ensemble method. It builds a series of decision trees sequentially, where each new tree corrects the errors of the previous ones. The final prediction is an aggregate of the predictions from all trees. The objective function that XGBoost minimizes combines a loss function and a regularization term:
$$\text{Obj} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)$$
where $l(y_i, \hat{y}_i)$ is the loss function that measures the error between the true label $y_i$ and the prediction $\hat{y}_i$, and $\Omega(f_k)$ is the regularization term that penalizes the complexity of the $k$-th tree to prevent overfitting.

\section{Model Interpretability Theory}
\label{sec:interpretability_theory}
To ensure the transparency and trustworthiness of the predictive models, this study will employ a state-of-the-art model explanation framework.

\subsection{SHAP (SHapley Additive exPlanations)}
SHAP is a game-theoretic approach used to explain the output of any machine learning model. It computes the contribution of each feature to a specific prediction. The core of SHAP is the additive feature attribution model, which represents an explanation as a linear function of binary variables:
$$g(z') = \phi_0 + \sum_{j=1}^{M} \phi_j z'_j$$
In this model, $g(z')$ is the explanation model that approximates the original model's output, $z' \in \{0, 1\}^M$ indicates the presence or absence of a feature, $M$ is the number of features, $\phi_0$ is the base value (the average prediction over the dataset), and $\phi_j$ is the \textbf{SHAP value} for feature $j$. This value represents the feature's contribution to pushing the model's output away from the base value.

A positive SHAP value for a feature indicates that the feature contributed to increasing the model's prediction, while a negative value indicates that it contributed to decreasing the prediction. The magnitude of the SHAP value indicates the strength of the feature’s impact.

\section{Personality Theory}

This research operationalizes personality using the \textit{Big Five Personality Traits} model \citep{costa1999five}, also known as the Five Factor Model (FFM). The Big Five personality model was formed over time through research from the idea that personality is rooted in our natural language. A lexical approach was used to extract thousands of relevant words from the English dictionary, which were then narrowed down by exploratory factor analysis, resulting in the final five traits \citep{feher2021looking}. The term “Big Five” was coined by Goldberg in 1981, referring to the model’s comprehensive coverage of personality. The Big Five model comprises the following personality traits:

\begin{enumerate}
	\item \textbf{Extraversion:} \\
	This trait describes an interest in other people and the external world. High-scoring individuals are more outgoing and energetic, while low scorers are described as more solitary and introspective. 
	\item \textbf{Agreeableness:} \\
 	This trait describes a person’s willingness to cooperate and compromise with others, or the ease with which they co-exist with other people. High-scoring individuals are seen as more accommodating, empathetic, and passive, while low scorers may appear argumentative and confrontational. 
	\item \textbf{Conscientiousness:} \\
	This trait describes the care and meticulousness put into one’s activities, relating to traits such as self-discipline and attention to detail.. High-scoring individuals tend to be organized and efficient, while low scorers may appear disorderly but more easygoing and spontaneous.
	\item \textbf{Neuroticism:} \\
	This trait describes a person’s tendency towards negative emotions and their emotional vulnerability to unfavorable scenarios. High-scoring individuals tend to be reactive and nervous, with their mental state being more easily influenced, while low scorers are emotionally stable, confident, and secure.
	\item \textbf{Openness to Experience:} \\
	This trait describes a person’s willingness to subject themselves to new, unfamiliar experiences and situations, and is also related to creativity. High-scoring individuals are curious, open-minded, and more open to trying new things, while low scorers are more cautious, preferring consistency and familiarity. 
\end{enumerate}

These five factors are considered statistically independent, meaning that a high or low score in one trait does not predict scores in the others \citep{babcock2020big}. There are numerous existing questionnaires that are used to assess these traits, such as the Big Five Inventory (BFI), the various versions of the NEO Personality Inventory, and the Big-Five Factor Markers. Research has also shown that these traits generally remain stable and consistent throughout adulthood \citep{babcock2020big}. However, there is an acknowledgement that while these five traits encompass a large area of personality, it does not claim to capture all aspects \citep{babcock2020big}. For instance, an alternative personality model, the HEXACO model, was developed based on the Big Five. It names a sixth dimension: Honesty-Humility, which was obtained by applying the lexical approach to multiple languages around the world. It also defines the base five traits slightly differently than the Big Five does \citep{feher2021looking}. Furthermore, it should be noted that the Big Five model was developed in a Western context, meaning that personality nuances of non-Western cultures may not be adequately captured with this model \citep{feher2021looking}.

\section{Dimensionality Reduction Theory}
\label{sec:theory_pca}
The feature extraction process, particularly for the image and text modalities, can generate feature vectors of high dimensionality. The ResNet50 model produces a 2,048-dimension vector for each image, while contextual embeddings from BERT models produce dense vectors of 768 dimensions. The direct use of such high-dimensional data in machine learning models presents a series of theoretical and practical challenges collectively known as the "curse of dimensionality" \citep{bellman1961, bellman1966}. Therefore, understanding dimensionality reduction techniques is necessary to address potential issues in the subsequent modeling stages.

The curse of dimensionality manifests in several critical ways:
\begin{enumerate}
	\item \textbf{Data Sparsity:} As the number of features (dimensions) increases, the volume of the feature space grows exponentially. With a finite dataset, the data points become increasingly sparse within this vast space. Consequently, the distance between any two points can become almost equal, rendering distance-based algorithms less effective and making it difficult for any model to identify statistically significant patterns from the limited training examples \citep{bellman1961, aggarwal2001}.
	
	\item \textbf{Increased Risk of Overfitting:} In a high-dimensional space, a model has more degrees of freedom, which dramatically increases its capacity to fit the noise and random fluctuations specific to the training data, rather than the true underlying relationship between features and personality traits. This phenomenon leads to models that perform well on training data but fail to generalize to new, unseen data, which would undermine the predictive utility of the final classifiers \citep{hastie2009}.
	
	\item \textbf{Computational Inefficiency:} Training machine learning models on feature vectors with thousands of dimensions is computationally expensive. It significantly increases memory requirements and processing time for training, validation, and hyperparameter tuning \citep{hastie2009}.
\end{enumerate}

Principal Component Analysis (PCA) is a primary method for addressing these challenges. This choice is predicated on PCA's specific properties, which align directly with the objectives of this study's feature engineering pipeline.
\begin{enumerate}
	\item \textbf{Preservation of Global Variance:} PCA is a linear transformation technique that projects the original data onto a new set of orthogonal axes, known as principal components. These components are ordered such that the first component captures the largest possible variance in the data, the second captures the second-largest variance, and so on. The fundamental goal of PCA is to reduce dimensionality while minimizing information loss, where "information" is quantified as statistical variance \citep{jolliffe2016}.
	
	\item \textbf{Unsupervised and Deterministic Nature:} PCA is an unsupervised algorithm; it computes the principal components based solely on the properties of the feature data itself, without any knowledge of the corresponding personality labels. This makes it a neutral preprocessing step that does not introduce bias from the target variables into the feature space. Furthermore, PCA is a deterministic algorithm, meaning that for a given dataset, it will always produce the same principal components and transformed data, which is essential for ensuring the reproducibility of research findings \citep{wold1987}.
	
	\item \textbf{Noise Reduction:} High-dimensional feature sets often contain a degree of noise or redundancy. The principal components associated with the lowest variance often correspond to this noise. By discarding these low-variance components, PCA effectively acts as a denoising filter, which can lead to more robust models that are less susceptible to spurious correlations in the data \citep{hastie2009}.
\end{enumerate}
