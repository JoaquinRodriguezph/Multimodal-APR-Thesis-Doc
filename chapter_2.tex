%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Filename    : chapter_2.tex 
%
%   Description : This file will contain your review of related works.
%                 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Works}
\label{sec:Related Works}


\section{Personality in Social Media Data}
\label{sec:Personality}
Research on automatic personality recognition (APR) has established robust links between digital footprints and personality traits, primarily leveraging text-based data from platforms like Twitter and Facebook. Foundational work by \citet{mairesse_using_2007} demonstrated that linguistic features (e.g., word choice, sentiment, syntactic complexity) correlate strongly with the Big Five traits, providing a framework for computational modeling. This was expanded by \citet{schwartz_personality_2013} whose open-vocabulary analysis of Facebook posts revealed nuanced patterns—such as extraverts using social activity language—highlighting social media's value as a behavioral lens. Subsequent studies confirmed cross-cultural applicability; for instance, \citet{adi_optimization_2018} and \citet{jeremy_automatic_2021} achieved high accuracy in Indonesian Twitter data using neural networks.

The advent of deep learning has refined text-based APR. \citet{majumder_deep_2017} utilized document-level neural networks to capture contextual cues, while \citet{han_knowledge_2020} developed interpretable models linking specific word categories to traits (e.g., weak correlations between neuroticism and supervision words). However, short-text platforms like Instagram pose challenges for traditional topic modeling \citep{albalawi_using_2020}. Crucially, studies in the Filipino context remain sparse. \citet{tighe_modeling_2018} pioneered this space, identifying code switching in Filipino Twitter users' language (e.g., TF–IDF unigrams most strongly predicted Conscientiousness). This work laid the groundwork for the PagkataoKo dataset \citep{tighe_acorda_2022}, a curated collection of Filipino social media data annotated with Big Five traits. While invaluable, PagkataoKo's Twitter-centric design limits insights into multimodal or platform-specific expressions on visually-oriented platforms.

Visual content offers untapped potential for personality inference. \citet{azucar_predicting_2018}'s meta-analysis synthesized evidence linking certain attributes to traits: activity statistics predict Extraversion, while demographic statistics predict Openness. Instagram-specific studies corroborate these findings; \citet{ferwerda_predicting_2018} demonstrated that HSV values and content(object recognition) modestly indicate personality although combining the two features does not result in an improvement. On the other hand \citet{branz_red_2020} found Openness correlated with warm hues (red/orange) and Conscientiousness with cooler tones (blue). These visual-semantic relationships remain underexplored in Filipino contexts, despite Instagram's dominance in Philippine social media landscapes \citep{gallardo_understanding_2024}.

Multimodal approaches significantly enhance APR accuracy by fusing complementary signals. \citet{batrinca_multimodal_2016} demonstrated that the combination of acoustic and visual non-verbal features enables statistically significant recognition of certain Big Five personality traits—such as Extraversion and Emotional Stability—particularly in goal-oriented collaborative tasks involving human-machine interactions. Similarly, \citet{lima_sequential_2022}'s LSTM-based fusion model outperformed unimodal baselines (text/audio) by 3.5\%  with LSTMs underperforming against GRU. Secuya (2021) while seemingly using multimodal data on their research, only uses TF-IDF Text Features and Twitter Account Features on their study. Furthermore, it overlooks Instagram's richer visual ecosystem where curated aesthetics \citep{harris_do_2019} yield stronger trait signals and depicted content, specifically human faces and skin \citep{machajdik_affective_2010} do yield stronger emotion signals.


\section{Text and Image Feature Extraction for Automatic Personality Recognition}

Automatic Personality Recognition (APR) from multimodal data, including text and images, has garnered significant attention in recent years due to its promising applications in understanding human behavior through online content. As the use of social media platforms like Instagram has grown, the extraction of personality-related features from users' posts has become an essential area of research. Recent studies have demonstrated that both text and image features, when combined, can enhance the performance of APR systems.

\subsection{Text Feature Extraction}

Text-based feature extraction in APR typically focuses on identifying linguistic cues that are indicative of personality traits. \citet{Mairesse2007} explored the use of various linguistic features, such as word categories, syntax, and semantic content, to predict the Big Five personality traits. Their study revealed that specific traits, such as Extraversion and Neuroticism, were strongly correlated with the use of certain types of words, such as positive or negative emotion terms. This aligns with findings from \citet{Pennebaker1999}, who identified linguistic markers such as first-person pronouns and emotional words as key indicators of personality.

With the advent of deep learning models, \citet{Christian2021} introduced a novel approach that utilizes pre-trained language models like BERT, RoBERTa, and XLNet for feature extraction, significantly improving the accuracy of text-based APR systems. These models capture the semantic meaning of words in context, which is crucial for understanding the nuances of social media posts. Furthermore, \citet{Albalawi2020} applied topic modeling techniques, specifically Latent Dirichlet Allocation (LDA), to extract latent topics from short-text data, showing that topics related to specific interests could be tied to personality traits.

\citet{Dzisevic_Sesok_2019} compared the performance of the variants of Term Frequency Inverse Document  Frequency (TF-IDF): default, Latent Semantic Analysis (LSA) and Linear Discriminant Analysis (LDA). They found that when larger datasets were inputted to the neural network, the default TF-IDF tended to perform better with higher validation accuracy, thus achieving 91\% accuracy. With classifier larger dataset, TF-IDF with LDA can have high validation accuracy.

While traditional text feature extraction methods such as TF-IDF are effective at identifying the importance of specific words within a document, they do not consider the contextual meaning or the sequence of words. In contrast, modern deep learning models like BERT (Bidirectional Encoder Representations from Transformers) analyze text bidirectionally, allowing them to understand the context of a word based on its surrounding words. This ability to capture nuanced semantic relationships has led to significantly better performance in tasks like personality classification. For example, a study directly comparing TF-IDF and a fine-tuned BERT model for personality prediction found that the BERT-based approach achieved higher accuracy \citep{Zhang_2023}. 


\subsection{Image Feature Extraction}

Visual content, including images, offers valuable insights into users' personality traits. Instagram, as a platform primarily focused on image-sharing, presents an excellent opportunity to explore how visual features can be linked to personality. \citet{Ferwerda2018} demonstrated that image features such as color (e.g., hue, saturation, brightness) and the presence of specific content (e.g., nature scenes, selfies) can be indicative of personality traits. For instance, users who score high in Extraversion tend to post brighter and more vibrant images, while those with higher Neuroticism scores tend to post darker and more subdued images.

Moreover, \citet{Branz2020} expanded on this by showing that the use of Instagram filters also correlates with personality traits, where extroverts are more likely to use filters that enhance brightness and contrast. Additionally, \citet{Reece2017} explored how computationally extracted features from Instagram images, including color analysis and face detection, could be used to predict depression, highlighting the potential for image-based predictions in mental health and personality analysis.

\citet{Amin_Sjarif_Yuhaniz_2025} outlined various points about CNN-based models, such as VGGNet, ResNet, DANs, FaceNet, and OpenFace for personality trait recognition. VGGNet is a deep CNN model that has up to 19 layers and uses 3x3 filters. It is used prominently in feature extraction with a cost of high computation power and large model size. ResNet offers better accuracy as it goes deeper network with great gradient flow. Its many variants like ResNet18, ResNet34, ResNet50 and ResNet101 offer great flexibility for different use cases. Its capability of skipping connections solves the issue of vanishing gradient. ResNet comes with a cost of using high number of parameters and long training time. Descriptor Aggregation Networks (DANs) are CNNs with average and max pooling layer replacing the fully connected layers \citep{Zhang_Zhang_Wei_Wu_2016}. The fully connected layers from DANs offer the benefits of reduced model size and final feature dimensionality, thus faster model training. However, it also results with less accurate feature information. FaceNet features deep CNN embeddings, which make it more accurate in face recognition and verification at the cost of triplet loss optimization during training \citep{Schroff_Kalenichenko_Philbin_2015}. Lastly, OpenFace is a lightweight successor of FaceNet with smaller model size \citep{Amos_Ludwiczuk_Satyanarayanan_2016}. It performs with lower accuracy compared to larger models such as VGGFace. 

\begin{comment}
To discuss the comparison between different image feature extraction, \citet{Qawasmeh_Oh_Kwigizile_2025} has provided a number of results according to the feature. For the feature "Road Type", AlexNet was proven to have the highest mean accuracy (84\%) and mean F-score (84\%) among VGG-19 and ResNet50. For the feature "Intersection Angle", AlexNet still holds the highest mean accuracy (85\%), and the highest mean F-score (84\%). \citet{Qiu_Lu_Wang_Chen_Chen_Yang_2024}'s study about image recognition of tomato leaf diseases had modified AlexNet model to fit the number of types of tomato leaf images. It beat the original AlexNet model modelled from \citet{Krizhevsky_Sutskever_Hinton_2017}, which has the accuracy of 87\%, recall of 93.24\%, precision of 92.57\% and F-score of 92.90\%. The modified AlexNet from \citet{Qiu_Lu_Wang_Chen_Chen_Yang_2024} has the highest accuracy of 98.72\%, recall of 98.91\%, precision of 99.77\% and F-score of 99.34\%.
\end{comment}

\subsection{Multimodal Feature Extraction}

Combining both text and image features has been shown to improve the accuracy of personality predictions. \citet{Batrinca2016} proposed a multimodal approach for personality recognition, integrating text and visual cues from social interactions. Their findings suggest that multimodal fusion techniques can capture more comprehensive personality profiles than using either modality alone. Similarly, \citet{Skowron2016} fused linguistic features from captions and hashtags with visual features extracted from images to predict personality traits. Their study confirmed that multimodal models outperform unisource models in personality prediction tasks.

Furthermore, \citet{Lima2022b} demonstrated the effectiveness of sequential models that dynamically weigh the importance of each modality (text and image) based on context, significantly improving the recognition of personality traits across various social media datasets.

\subsection{Feature Extraction in the Context of Instagram}

Studies on Instagram, like that of \citet{Ferwerda2016}, have shown that image features such as color, saturation, and brightness are strongly tied to personality traits. However, these studies focus solely on image features and do not combine text and image modalities. This highlights the importance of distinguishing between unimodal image-based approaches and true multimodal integration, where both text and image features are used together for more robust predictions.

In contrast, \citet{Xianyu2016} applied their own multimodal learning technique (Hetero
geneity Entropy Neural Network (HENN)) to integrate both textual and visual features which outperforms the baseline systems significantly (CCA-based and Corr-AE method).

\section{Multimodal Approaches to APR}
\label{sec: MMApproaches}
Multimodal APR uses different data sources, or modalities, which offer complementary information. Visual cues from images may reveal aesthetic preferences and social tendencies, linguistic patterns in text may expose cognitive styles and emotional states, and behavioral metadata can quantify social engagement. The fusion of these signals can lead to more robust, reliable, and nuanced personality models.

\cite{batrinca_multimodal_2016} demonstrated that the fusion of simple acoustic features, such as pitch and intensity, with basic visual features like motion vectors, results in a statistically significant improvement in the recognition of traits like Extraversion and Emotional Stability. Their research displayed that certain traits manifest more prominently through specific channels; for instance, Extraversion is often correlated with increased vocal intensity and physical movement, whereas Emotional Stability can be inferred from vocal tone consistency, especially under induced stress from non-collaborative interactions. This is further supported by \cite{azucar_predicting_2018} wherein Facebook metadata (e.g., number of posts, number of
friends or network density, number of received Likes) where linked to Extraversion and certain demographics (gender and age) linked to Openness.

Various strategies are used for combining multiple data modalities namely: feature-level (early) fusion, decision-level (late) fusion, and model-level (intermediate) fusion.  

Feature-level fusion is the most direct approach to multimodal integration. It involves extracting feature vectors from each modality and concatenating them into a single, high-dimensional vector which is then used as an input to a machine learning model for prediction. An early example in the APR domain is a study by \cite{Sidorov2014} who combined audio and visual features at the feature level for personality trait classification. The most significant challenge that early fusion presents is a  phenomenon often referred to as the \textit{curse of dimensionality}, which increases the risk of overfitting and demands larger datasets for effective training. 

In contrast to feature-level fusion, decision-level fusion involves training separate, independent models for each modality. The predictions from these unimodal models are then combined at the final stage to produce a single output, typically through methods like averaging, voting, or a weighted sum. A recent example of this is a research by \cite{salam22a} who utilized Neural Architecture Search (NAS) to automatically design and train distinct deep learning models for visual, audio, and text modalities, tailored to specific user profiles based on gender and age. While it allows for the use of the most suitable and highly optimized architecture for each data type and is inherently robust to missing modalities, the main limitation of  decision-level fusion is that it fails to leverage the correlations between modalities at the feature level, which can result in the loss of subtle interaction cues that might be crucial for accurate personality assessment.

Lastly is the model-level fusion which involves integrating information at an intermediate stage within the model architecture itself. This model-level fusion allows for interaction between modalities after initial feature extraction but before the final prediction layer. A recent study by \cite{Bhin2025} exemplifies this where their model first processes audio (Wav2Vec2), visual (Skeleton Landmarks), and text (BERT) data through separate, modality-specific streams. The resulting context-aware representation vectors for each modality are then combined via a simple late fusion step (concatenation) before being passed to the final classification layers. 

While recent APR research has readily adopted deep learning models like BERT and Wav2Vec2 for unimodal feature extraction, the methods used to integrate these powerful representations often remain limited to simple strategies like feature concatenation or decision averaging \citep{Bhin2025, Zhao2022}. This disparity between the sophisticated feature extraction techniques and the simple feature fusion highlights an opportunity to tackle this gap in APR research. Other computational fields that analyze complex, human-centric data, such as Human Activity Recognition (HAR) and computational health, have developed more advanced fusion techniques to address challenges that are directly analogous to those in APR. \citep{Ehatisham-Ul-Haq_Javed_Azam_Malik_Irtaza_Lee_Mahmood_2019, Cai_Qu_Li_Zhang_Hu_Hu_2020}.

The challenge of modality dominance—where high-dimensional features (e.g., from images) numerically overwhelm low-dimensional ones (e.g., metadata)—is a critical issue in early fusion that may be overlooked in APR \citep{Ehatisham-Ul-Haq_Javed_Azam_Malik_Irtaza_Lee_Mahmood_2019}. Adopting rigorous feature normalization and balancing strategies, as seen in HAR, would ensure that all data streams contribute meaningfully to the final prediction. Secondly, APR could move beyond static fusion by employing engineered fusion and adaptive weighting \citep{Cai_Qu_Li_Zhang_Hu_Hu_2020}. Instead of just combining features, new "engineered" features could be created to represent the interaction or even the discrepancy between modalities. Furthermore, adaptive weighting techniques, implemented through methods like Genetic Algorithms or cross-modal attention, could allow a model to learn the dynamic, trait-specific importance of each modality. Integration of these approaches presents as opportunities to build more accurate and interpretable personality recognition models.

\section{Filipino APR}
\label{sec: FilipinoAPR}
Filipino APR remains an underexplored domain. Existing APR research is mostly focused on English-speaking populations although there is a rise of studies that have developed specific models for different languages, namely Bahasa Indonesian, Chinese, and Egyptian \citep{Siddique2019, Salem2019, Adi2018}. What makes Filipino APR distinct from these other languages is the presence of multiple languages, some of which include an array of unique regional languages \citep{tighe_modeling_2018}. Moreover, code-switching is prominent in multilingual people which adds another layer of complexity.

The first venture to Filipino APR was conducted by \citet{tighe_modeling_2018} wherein they analyzed linguistic data from 250 Filipino Twitter users. Due to there being a lack of Filipino APR datasets, \citet{tighe_acorda_2022} constructed the PagkataoKo dataset, which contains data from 3,128 Filipino Twitter/Instagram users and now consists of both text and image data. Since then, several studies have been conducted using the PagkataoKo dataset, but only one of which has so far integrated image data. Table~\ref{tab:filipino_apr_summary} presents an overview of works conducted in the field of Filipino APR. It can be seen that there is a heavy focus on text data, particularly those sourced from X (formerly Twitter), as well having a general tendency toward unimodal approaches. The RMSE and $R^2$ metrics refer to the best model produced.

\begin{table}[ht]
\scriptsize
\centering
\caption{Summary of Filipino APR Studies}
\label{tab:filipino_apr_summary}
\begin{tabularx}{\textwidth}{@{}l l l l l l@{}}
\toprule
\textbf{Study} & \textbf{Authors} & \textbf{Platform} & \textbf{Modality} & \textbf{RMSE} & \textbf{R\textsuperscript{2}} \\
\midrule
Filipino Twitter Users & Tighe and Cheng (2018) & Twitter & Text & 0.1523 & 0.1523 \\
Neural network approach & Tighe et al. (2020) & Twitter & Text & 0.3344 & 0.2799 \\
Content analysis & Acorda et al. (2019) & Instagram & Image & 0.500 &  \\
Multimodal & Secuya (2021) & Twitter & Text, metadata & 0.4403 & 0.2021 \\
Word Embeddings & Chua Chiaco et al. (2022) & Twitter & Text & 0.6087 & -0.0061 \\
Topic Modelling & Algenio et al. (2023) & Twitter & Text &  & 0.0982 \\
User Embedding w/ Transformers & Burguilos et al. (2023) & Twitter & Text & 0.4751 & 0.0652 \\
Structuring Social Media Posts & Alvarez et al. (2024) & Twitter & Text &  & 0.11 \\
Predict Questionnaire Responses & Gomez et al. (2024) & Twitter & Text & 0.6270 & 0.1240 \\
\bottomrule
\end{tabularx}
\end{table}

This paper seeks to contribute to Filipino APR by exploring the effects of fusing image data as a feature. Fusion and information extraction techniques from previous multimodal studies would serve as a guide for this research’s methodology.


%Previous draft
%\citet{Pennebaker1999} explored on their study how language use can reflect the personality style of someone. Using a computerized word-based text analysis program, the structure, validity, and reliability of the written language was examined. From there it was found that one's linguistic style is a consequential way to explore personality.

%This was further explored in a study by \citet{Mairesse2007} in which the recognition of the Big Five Personality traits (Openness, Conscientiousness, Extroversion, Agreeableness, and Neuroticism) on both text and conversation was first experimented using both self rating and observer ratings of personality. From the experimentation done with the various models: classification, ranking, and regression; the ranking models performed best overall.

%The PagkataoKo dataset analyzed in this paper has been used in the studies of undergraduates, graduates, and professionals at DLSU. These various studies primarily involved modeling Filipino users’ personality traits or Automatic Personality Recognition (APR) through their usage of the Instagram and X (formerly Twitter) social media sites, as was the purpose of the creation of the dataset. 

%The dataset contains data on Filipino social media users’ social media activity on Instagram and Twitter. This includes features like post images and captions,  profile pictures, follower and following counts, and post count. It also includes the user’s BFI scores.

%Among the undergraduate and graduate studies that pursued the analysis of this dataset, the majority of them utilized only the X data, focusing on natural language processing. Techniques such as word embedding models, topic modeling, feature extraction, and other models were used to predict the BFI personality traits of users. One used both X and Instagram, but only the text data was used. There also exists another study that analyzed the visual features of Instagram image data and used this to predict personality traits. Lastly, a published study also used text processing on the X data for the same purpose.

%X was a more popular choice for analysis as the platform's main focus is that it is more text-based and the captions hold the most importance whereas on Instagram, it is more secondary. This leaves the Instagram portion of the dataset relatively unexplored compared to the X data, so there is little information about image processing in this dataset. However, this leaves open the potential for exploring images and their relation to personality, specifically the BFI scores, adding to the one research that was done on this topic. 

%Filipinos use social media in many ways. They use it for connecting with other people, sharing information, speaking out, and optimizing productivity \citep{10.1007/978-3-031-61543-6_24}. However, even though they are aware of the presence of social media, \citet{Cruz_Jamias_2013} found that Filipino researchers rarely use social media to their advantage. This demonstrates that each demographic in the Philippines has a different level of social media use.

%Social media has many effects in the Philippines. For instance, it plays a role in youth political participation. \citet{Ibardeloza2022-pz} found that social media helps the youth to be more exposed to radical involvement. It also has a positive effect on “peer interpersonal relationships” yet slightly negative on familial relationships \citep{Bristol2016TheDM}.

%Recent studies have explored the potential of Instagram image data to infer users' personality traits. \citet{ferwerda2016} analyzed features such as hue, brightness, and saturation in users' photos, finding significant correlations between these visual elements and the Big Five personality traits. Their findings suggest that individuals' choices in photo appearance can reflect underlying personality characteristics. 

%Further research by \citet{Reece2017-qw} utilized machine learning to examine Instagram photos for markers of depression. By analyzing color properties, metadata, and the presence of faces, their model successfully identified depressive indicators, highlighting the platform's potential for mental health screening. 

%Additionally, a study by \citet{Harris2019-gq} investigated the accuracy of personality portrayal on Instagram by comparing observers' perceptions of account holders' personalities with the account holders' self-reported traits. The results indicated discrepancies, suggesting that while certain visual cues can convey personality aspects, they may also lead to misinterpretations. 

%Collectively, these studies underscore the viability of leveraging Instagram image data to assess users' personality traits and mental health status, though they also caution against overreliance on visual cues due to potential misrepresentations.


%%This chapter provides a synthesis of past research, existing algorithms, and or state-of-the-art software that are related/similar to the thesis. It should not present detailed summaries of each related work, but rather present a cohesive comparison of different aspects of their work. At the end of each section and this chapter, it should be clear what research challenges and opportunities will be focused on for the proposal.

%%The sections can be about approaches, application areas, and categories of solutions that give readers an deep understanding of the current state of the field. 

%%Observe a consistent format when presenting each of the reviewed works. This must be selected in consultation with the prospective adviser.

%%\textcolor{red}{DO NOT FORGET to cite your references.} Related works can be discussed multiple times in different sections of this chapter, depending on what is being discussed or compared.


\begin{comment}
%
% IPR acknowledgement: the contents within this comment are from Ethel Ong's slides on RRL.
%
Guide on Writing your Related Works chapter
 
1. Identify the keywords with respect to your research
      One keyword = One document section
                Examples: 2.1 Story Generation Systems
			 2.2 Knowledge Representation

2.  Find references using these keywords

3.  For each of the references that you find,
        Check: Is it relevant to your research?
        Use their references to find more relevant works.

4. Identify a set of criteria for comparison.
       It will serve as a guide to help you focus on what to look for

5. Write a summary focusing on -
       What: A short description of the work
       How: A summary of the approach it utilized
       Findings: If applicable, provide the results
        Why: Relevance to your work

6. At the end of each section,  show a Table of Comparison of the related works 
   and your proposed project/system

\end{comment}
















