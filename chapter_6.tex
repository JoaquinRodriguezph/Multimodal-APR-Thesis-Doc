\chapter{Conclusion and Recommendations}

\section{Conclusion}

This research investigated the relationship between Instagram activity and the Big Five personality traits using the PagkataoKo dataset. By finalizing a specific subset of data points and identifying key characteristics for Automatic Personality Recognition, the study performed feature extraction through a multimodal lens. Visual elements were captured via Imagga, ResNet50, and HSV; textual data was processed through TF-IDF and Word2Vec; and behavioral metadata was included via post and following counts. These features were then used to train and compare Logistic Regression, Support Vector Machines, and XGBoost models.

The results demonstrate that the effectiveness of personality prediction is highly trait-dependent. Textual modalities such as TF-IDF and Word2Vec emerged as the primary drivers for predicting Conscientiousness, Agreeableness, and Neuroticism. In contrast, visual modalities like ResNet and HSV dominated the prediction of Openness and Extraversion. Furthermore, the inclusion of metadata served as a crucial refinement for Agreeableness and Conscientiousness, capturing behavioral regularities that content alone could not model. While deep learning-based features provided the highest accuracy, they offered less transparency than interpretable methods like TF-IDF. Ultimately, while a multimodal approach generally enhances performance, certain traits are still most effectively expressed through unimodal features.

\section{Recommendations}
\label{sec:recommendations}

Future research should continue to evolve by exploring a mixed-feature direction rather than relying on a single modality. While this study identified that some traits rely on specific channels, others perform better when traditional features are combined with deep visual cues. Pushing multimodal approaches is highly recommended because user traits often manifest across multiple channels simultaneously. Regarding model architectures, the findings suggest a transition toward more complex systems. While the current dataset size might make standard Multilayer Perceptrons (MLPs) less effective, the use of multimodal fusion networks is a promising path for capturing non-linear relationships between visual and textual data.

Specific technical improvements can also be made to the feature extraction process. For textual data, exploring Convolutional Neural Networks (CNNs) could capture local context more effectively and push the performance of Word2Vec embeddings further. Additionally, since the domain of social media data often differs from standard training sets, exploring other pre-trained transformers could bridge this gap. On the visual side, while ResNet provided strong semantic features, the Imagga tags yielded negligible improvements. Future work should look into alternative objective detection algorithms or open-source visual models, especially since Imagga is a private tool. Finally, researchers should prioritize computational efficiency by streamlining feature selection and focusing on high-performing markers like ResNet, TF-IDF, and Word2Vec while pruning lower-impact metadata.